{
  "hash": "8b25002b94c68e3487387fd65adde993",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Deep Learning with Keras :: Cheatsheet\"\ndescription: \" \"\nimage-alt: \"\"\nexecute:\n  eval: false\n  output: true\n  warning: false\n---\n\n\n::: {.cell .column-margin}\n<img src=\"images/logo-keras3.png\" height=\"138\" alt=\"Hex logo for keras3. It is a very large white K with a red background. It has the name of the package in smaller letters on the right.\" />\n<br><br><a href=\"../keras.pdf\">\n<p><i class=\"bi bi-file-pdf\"></i> Download PDF</p>\n<img src=\"../pngs/keras.png\" width=\"200\" alt=\"\"/>\n</a>\n<br><br><p>Translations (PDF)</p>\n* <a href=\"../translations/chinese/keras_zh_cn.pdf\"><i class=\"bi bi-file-pdf\"></i>Chinese</a>\n* <a href=\"../translations/japanese/keras_ja.pdf\"><i class=\"bi bi-file-pdf\"></i>Japanese</a>\n* <a href=\"../translations/spanish/keras_es.pdf\"><i class=\"bi bi-file-pdf\"></i>Spanish</a>\n:::\n\n\n## Intro\n\n**Keras** enables fast experimentation with \"neural networks\".\nIt supports **convolution networks** (vision) and **recurrent networks** (text and time series).\nIt provides the freedom to x work with *JAX*, *Tensorflow*, and *Torch*, plus the freedom to build models that can seamlessly move across these frameworks.\n**keras3** provides easy access to the Keras vast API.\nIt also makes getting started with Keras faster by automatically creating a working Python environment with all the needed libraries pre-installed.\n\n## Model building\n\n### 1. Define the model\n\n- `keras_input()` / `keras_model()`: Defines a Functional Model with inputs and outputs.\n\n    ```r\n    inputs <- keras_input(<input-shape>)\n    outputs <- inputs |> \n      layer_dense() |> \n      layer_…\n    model <- keras_model(inputs, outputs)\n    ```\n- `keras_model_sequential()`: Define a Sequential Model composed of a linear stack of layers\n\n    ```r\n    model <- keras_model_sequential(<input-shape>) |> \n      layer_dense() |> \n      layer_...\n    ```\n\n- `Model()`: Subclass the base Model class\n\n### 2. Inspect the model\n\n- `summary(<model>)`: A summary of a Keras Model\n\n- `plot(<model>)` Plot the model. Needs **graphviz** to work:\n[graphviz.gitlab.io/download](https://graphviz.gitlab.io/download/)\n\n### 3. Compile the model\n\n- `compile()`: Configure aspects of the model such as optimizer, loss, metrics, \nweights and others.\n\n    ```r\n    model |> compile(\n      loss = \"categorical_crossentropy\", \n      optimizer = \"rmsprop\", \n      metrics = \"accuracy\"\n      )\n    ```\n    \n### 4. Fit the model\n\n- `fit()`: Trains the model for a fixed number of dataset iterations (epochs)\n\n    ```r\n    model |> \n      fit(<x>, <y>, epochs = 30, batch_size = 128, validation_split = 0.2)\n    ```\n\n\n### 5. Evaluate the model\n\n- `evaluate(<model>)`: Returns the loss and metrics\n    \n    ```r\n    model |> \n      evaluate(<x test>,<y test>)\n    ```\n    \n- `predict(<model>, x)`: Generates predictions\n\n## Example\n\nTrain image recognizer on MNIST data a.k.a. deep learning’s ‘hello world’ example \n\n```r\nlibrary(keras3)\n\n# Input layer: use MNIST images\n.[.[x_train, y_train], .[x_test, y_test]] <- dataset_mnist()\n\n# Rescale and categorize\nx_train <- x_train/255\nx_test <- x_test/255\nnum_classes <- 10\ny_train <- to_categorical(y_train, num_classes)\ny_test <- to_categorical(y_test, num_classes)\n\n# Define the model and layers\nmodel <- keras_model_sequential(\n  input_shape = c(28, 28, 1)) |>\n  layer_conv_2d(\n    filters = 32, kernel_size = c(3, 3),\n    activation = \"relu\") |>\n  layer_max_pooling_2d(pool_size = c(2, 2)) |>\n  layer_conv_2d(\n    filters = 64,\n    kernel_size = c(3, 3),\n    activation = \"relu\") |>\n  layer_max_pooling_2d(pool_size = c(2, 2)) |>\n  layer_flatten() |>\n  layer_dropout(rate = 0.5) |>\n  layer_dense(units = num_classes, activation = \"softmax\")\n  \n# Inspect the model\nsummary(model)\n\nplot(model)\n\n# Compile (define loss and optimizer)\nmodel |>\n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = optimizer_rmsprop(),\n    metrics = c(\"accuracy\")\n    )\n    \n# Fit the model\nmodel |> \n  fit(\n    x_train, y_train, \n    epochs = 30, \n    batch_size = 128, \n    validation_split = 0.2\n    )\n  \n# Evaluate and predict\nmodel |> \n  evaluate(x_test, y_test)\n  \nmodel |>\n  predict(x_test)\n  \n# Save the full model\nsave_model(model, \"mnist-classifier.keras\")\n\n# Deploy for serving inference\ndir.create(\"mnist\")\nexport_savedmodel(model, \"mnist/1\")\nrsconnect::deployTFModel(\"mnist\")\n```\n\n## Layers\n\n### Core layers\n\n- `layer_dense()`: A layer connected to all neurons in the preceding layer\n\n- `layer_einsum_dense()`: A dense layer of arbitrary dimensionality\n\n- `layer_embedding()`: Acts as a mapping function, it stores a dense vector for each word \nin the vocabulary\n\n- `layer_lambda()`: Allows arbitrary expressions to be used as a layer\n\n- `layer_masking()`: Masks a sequence by using a mask value to skip time steps\n\n### Convolution layers \n\nLayers that create a convolution kernel that is convolved with the layer input \nover one, two, or three dimensions to produce a tensor of outputs.\n\n- `layer_conv_1d()` / `layer_conv_1d_transpose()`:  Layer of a single dimension\n(temporal). Transpose does the opposite, deconvolution.\\\n\n- `layer_conv_2d()` / `layer_conv_2d_transpose()`:  Two dimensional layer\n(image). Transpose does the opposite, deconvolution.\n\n- `layer_conv_3d()` / `layer_conv_3d_transpose()`:  Three dimensional layer\n(images over volumes). Transpose does the opposite, deconvolution.\n\n- `layer_depthwise_conv_1d()` / `layer_depthwise_conv_2d()`: A type of \nconvolution in which each input channel is convolved with a different kernel.\n\n- `layer_separable_conv_1d()` / `layer_separable_conv_2d()`: A depthwise \nconvolution that acts separately on  channels, followed by a pointwise\nconvolution that mixes channels.\n\n- `layer_depthwise_conv_1d()` / `layer_depthwise_conv_2d()`: A type of \nconvolution  in which each input channel is convolved with a different kernel.\n\n- `layer_separable_conv_1d()` / `layer_separable_conv_2d()` - A depthwise \nconvolution  that acts separately on channels, followed by a pointwise\nconvolution that mixes channels.\n\n### Normalization layers\n\n- `layer_batch_normalization()`: Operates across the batch dimension\n\n- `layer_layer_normalization()`: Operates across the feature dimension\n\n- `layer_group_normalization()`: Operates across channels\n\n- `layer_spectral_normalization()`: Controls the Lipschitz constant of the weights\n\n- `layer_rms_normalization()`: Root Mean Square\n\n### Regularization layers\n\n- `layer_batch_normalization()`: Maintains the mean output close to 0 and the\noutput standard deviation close to 1.\n\n- `layer_gaussian_noise()`: Layers that randomly \"drop\" a fraction of input \nunits during training by setting them to 0\n\n- `layer_dropout()`: Non 0 inputs are scaled up\n\n- `layer_alpha_dropout()`: Keeps original mean and variance\n\n- `layer_gaussian_dropout()`: 1-centers Gaussian noise\n\n- `layer_spatial_dropout_1d()` / `layer_spatial_dropout_2d()` / \n`layer_spatial_dropout_3d()`: Drops entire dimensional feature maps instead of \nindividual elements\n\n### Pooling layers\n\nLayers that **reduce** each dimension of the input while retaining the most important\nfeatures\n\n- `layer_max_pooling_1d()` / `layer_max_pooling_2d()` / `layer_max_pooling_3d()`:\nMaximum value over a specified window size (pool size)\n\n- `layer_average_pooling_1d()` / `layer_average_pooling_2d()` / \n`layer_average_pooling_3d()`: Average value over a specified window size \n(pool size)\n\n- `layer_global_max_pooling_1d()` / `layer_global_max_pooling_2d()` /\n`layer_global_max_pooling_3d()`: Maximum value of the dimension\n\n- `layer_global_average_pooling_1d()` / `layer_global_average_pooling_2d()` /\n`layer_global_average_pooling_3d()`: Average value of the dimension\n\n## Preprocessing layers\n\n### Numerical features\n\n- `layer_normalization()`: Normalizes continuous features\n\n- `layer_discretization()`:  Buckets by ranges\n\n### Categorical features\n\n- `layer_category_encoding()`: Encodes integer features\n\n- `layer_hashing()`: Hashes and bins features\n\n- `layer_hashed_crossing()`: Crosses features using the \"hashing trick\"\n\n- `layer_string_lookup()` / `layer_integer_lookup()`: Maps a set of arbitrary \ntext or integers via a table-based vocabulary lookup\n\n\n### Data frames\n- `layer_feature_space()`: Main function to specify the how the features are \nto be pre-processed\n\n    ```r\n    feature_space <- layer_feature_space(\n      features = list(float_var = feature_float_normalized(),\n      string_var = feature_string_categorical(),\n      int_var = feature_integer_categorical())\n      )\n    ```\n\n- `feature_float()`\n\n- `feature_float_rescaled()`\n\n- `feature_float_normalized()`\n\n- `feature_float_discretized(num_bins)`\n\n- `feature_integer_categorical()`\n\n- `feature_string_categorical()`\n\n- `adapt()`: Calculates the the normalizations, bins, and other conversions \nagainst the data set \n\n    ```r\n    adapt(feature_space, <TF dataset>)\n    ```\n    \n### Text\n- `layer_text_vectorization()` / `get_vocabulary()` / `set_vocabulary()`: Maps \na set of arbitrary text or integers via a table-based vocabulary lookup\n\n### Audio\n\n- `layer_mel_spectrogram()`: Converts signal to Mel\n\n### Images \n\n- `layer_resizing()`: Changes the size of images\n\n- `layer_rescaling()`: Multiplies scale by a given number\n\n- `layer_center_crop()`: Crops images to the given size\n\n- `layer_auto_contrast()`: Makes differences between pixels more obvious    \n\n### Image augmentation\n\n|||\n|-------------------------|-------------------------|\n|`layer_aug_mix()` |`layer_random_hue()`|\n|`layer_cut_mix()` |`layer_random_invert()`|\n|`layer_mix_up()`|`layer_random_rotation()`|\n|`layer_solarization()`|`layer_random_shear()`|\n|`layer_random_contrast()`|`layer_random_zoom()`|\n|`layer_random_crop()`|`layer_random_saturation()`|\n|`layer_random_erasing()`|`layer_random_sharpness()`|\n|`layer_random_flip()`|`layer_random_translation()`|\n|`layer_random_grayscale()`|`layer_random_brightness()`|\n|`layer_random_color_jitter()`|`layer_random_gaussian_blur()`|\n|`layer_random_perspective()`|`layer_random_color_degeneration()`|\n|`layer_random_posterization()`||\n\n## Data Loading\n\n- `text_dataset_from_directory()`\n\n- `audio_dataset_from_directory()`: Reads .wav files\n\n- `image_dataset_from_directory()`: This function supports JPEG, Bitmap, PNG and GIF\n\n- `image_load()`: Loads image into a PIL format\n\n- `image_from_array()`: Converts array into PIL format\n\n- `image_to_array()`: Converts PIL image to array \n\n- `timeseries_dataset_from_array()`: Creates a dataset of sliding windows over \na time series\n\n- `pad_sequences()`: Pads sequences to the same length\n\n## Save and Deploy\n\n### I/O operations\n\n- `save_model(\"<path>\")` / `load_model(\"<path>\")`: Manage models using the \n\".keras\" file format.\n- `save_model_weights()` / `load_model_weights()`:  Manage model weights to/from \n\".h5\" files.\n\n- `save_model_config()` / `load_model_config()`:  Manage model architecture \nto/from a \".json\" file.\n\n### Deploy\n\n- `export_savedmodel(<model>, \"<path>/1\")`: Save a TF SavedModel for inference.\n\n- `rsconnect::deployTFModel(\"<path>\")`- Deploy a TF SavedModel to Posit Connect\nfor inference.\n\n## Operations\n\nIntroduced in Keras v.3, these are low-level operations that will work the same\nin *JAX*, *TF* and *Torch*. \n\n### Core\n\n- `op_associative_scan()`: Faster `op_scan()` if the function performs a binary \nassociative operation\n\n- `op_cast()`: Casts to specified dtype\n\n- `op_cond()`: Conditionally applies one of two functions\n\n- `op_convert_to_array()`: Tensor to R array\n\n- `op_convert_to_numpy()`:  Tensor to Numpy array\n\n- `op_convert_to_tensor()`:  Array to tensor\n\n- `op_custom_gradient()`: Allows fine grained control over the gradients of\na sequence for operations.\n\n- `op_dtype()`: Returns tensor’s dtype\n\n- `op_fori_loop()`: For loop, return tensor\n\n- `op_is_tensor()`: Checks for backend specific tensor\n\n- `op_map()`: Applies a function to every single value in the tensor\n\n- `op_rearrange()`: Rearranges tensor to specification\n\n- `op_scan()`: Maps a function, but retaining state\n\n- `op_scatter()`:  Modifies tensor in specified locations with zeroes\n\n- `op_scatter_update()`: It modifies tensor in specified locations that do \nnot need to be contiguous\n\n- `op_searchsorted()`: Performs a binary search\n\n- `op_shape()`: Returns tensor’s shape\n\n- `op_slice()`:  Extract specific location within the tensor\n\n- `op_slice_update()`:  It modifies tensor in a specified location\n\n- `op_stop_gradient()`: Stops gradient computation\n\n- `op_subset()`: Get specific section of the tensor\n\n- `op_switch()`: Applies one of several functions passed to the function based \non an index argument\n\n- `op_unstack()`: Splits tensor into a list of tensors separated along the \nspecified axis\n\n- `op_vectorized_map()`: Applies a function to every single value in the tensor. \nDesigned for vectorized operations\n\n- `op_while_loop()`: While loop implementation\n\n### Images\n\n- `op_image_affine_transform()`: Applies the given transform\n\n- `op_image_crop()`: Crops to specified height and width\n\n- `op_image_gaussian_blur()`: Applies Gaussian blur\n\n- `op_image_extract_patches()`: Extracts from image\n\n- `op_image_hsv_to_rgb()`: Converts from HSV to RGB\n\n- `op_image_map_coordinates()`: Maps input array to new coordinates by \ninterpolation\n\n- `op_image_pad()`: Pad images with zeroes to specified height and width\n\n- `op_image_perspective_transform()`: Applies a perspective transformation to \nthe image\n\n- `op_image_resize()`: Resize image to size using the specified interpolation \nmethod\n\n- `op_image_rgb_to_grayscale()`: Converts from RGB to grayscale\n\n- `op_image_rgb_to_hsv()`:  Converts from RGB to HSV\n\n### Neural network\n\n\n|||\n|-------------------------|-------------------------|\n|`op_average_pool()`|`op_batch_normalization()`|\n|`op_binary_crossentropy()`|`op_celu()`|\n|`op_conv()`|`op_conv_transpose()`|\n|`op_ctc_loss()`|`op_depthwise_conv()`|\n|`op_elu()`|`op_gelu()`|\n|`op_glu()`|`op_hard_shrink()`|\n|`op_hard_sigmoid()`|`op_hard_silu()`|\n|`op_hard_tanh()`|`op_leaky_relu()`|\n|`op_log_sigmoid()`|`op_log_softmax()`|\n|`op_max_pool()`|`op_moments()`|\n|`op_multi_hot()`|`op_categorical_crossentropy()`|\n|`op_sparse_categorical_crossentropy()`|`op_normalize()`|\n|`op_one_hot()`|`op_polar()`|\n|`op_psnr()`|`op_relu()`|\n|`op_relu6()`|`op_rms_normalization()`|\n|`op_selu()`|`op_separable_conv()`|\n|`op_sigmoid()`|`op_silu()`|\n|`op_soft_shrink()`|`op_softmax()`|\n|`op_softplus()`|`op_softsign()`|\n|`op_sparse_plus()`|`op_sparsemax()`|\n|`op_squareplus()`|`op_tanh_shrink()`|\n|`op_threshold()`|`op_unravel_index()`|\n\n\n## Deep Learning with R Book 3rd Edition\n\n[https://www.manning.com/books/deep-learning-with-r-third-edition](https://www.manning.com/books/deep-learning-with-r-third-edition)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}